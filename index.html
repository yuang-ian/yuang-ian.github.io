<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuang Shi</title>
  
  <meta name="author" content="Yuang Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<p style="text-align:center;font-size:small;">
  Last updated: <a id="demo"></a>.
</p>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuang Shi (ÊñΩÂÆáÊòÇ)</name>
              </p>
              <p style="text-align:justify">
                I'm a third year Ph.D. Candidate at National University of Singapore advised by <a href="https://www.comp.nus.edu.sg/cs/people/ooiwt/">Prof. Wei Tsang Ooi</a>, where I work on Networking and Multimedia Systems, specifically on 3D Media Streaming.
              </p>
              <p style="text-align:justify">
                I received my M.Comp Degree in 2022 from National University of Singapore and B. Eng. Degree in 2021 from Sichuan University in Sichuan.
              </p>
              <p style="text-align:justify">
                I am a part-time cat person and a full-time dog person. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yuangshi@u.nus.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=ytwibHUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/Shiyuang-scu">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SHIyuang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/SHIyuang_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <ul></ul>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                  <ul>
                    <li>
                      [2025/04] - Our paper "LTS: A DASH Streaming System for Dynamic Multi-Layer 3D Gaussian Splatting Scenes" received the <b style='color:red;'>Best Paper Award</b> at MMSys 2025!
                    </li>
                    <li>
                      [2025/01] - Our paper "LTS: A DASH Streaming System for Dynamic Multi-Layer 3D Gaussian Splatting Scenes" is accepted to MMSys 2025.
                    </li>
                    <li>
                      [2024/11] - Our paper "LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming" is accepted to 3DV 2025.
                    </li>
                    <li>
                      [2024/08] - Our <a href="https://dl.acm.org/doi/10.1145/3672196.3673394" target="_blank">paper</a> received the <b style='color:red;'>Best Paper Award</b> at the SIGCOMM 2024 Workshop on Emerging Multimedia Systems (EMS 2024)! See the <a href="https://www.comp.nus.edu.sg/bytes/our-faculty-and-phd-student-win-best-paper-at-sigcomm-2024/" target="_blank">Press</a> from NUS.
                    </li>
                    <li>
                      [2024/05] - Invited talk at Sichuan University, Sichuan.
                    </li>
                    <li>
                      [2024/04] - Invited talk at Universit√© de Toulouse, Toulouse INP-ENSEEIHT, IRIT, France.
                    </li>
                    <!-- <li>
                      [2024/01] - Our paper "QV4: QoE-based Viewpoint-Aware V-PCC-encoded Volumetric Video Streaming" is accepted to MMSys 2024.
                    </li> -->
                    <!-- <li>
                      [2023/12] - Invited <a href="https://dcs.site.nthu.edu.tw/p/406-1174-260551,r67.php" target="_blank">talk</a> at National Tsing Hua University, Taiwan.
                    </li> -->
                  </ul>
              </td>
            </tr>
          </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                  As a PhD student, I mainly focus on 3D Media Streaming, including compression, networking, and quality evaluation. 
                </p>
                <p>
                  Some works are <span class="highlight">highlighted</span>. 
                  Authors marked with <sup>*</sup> are interns or students whom I mentored when the work was carried out.
                </p>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            
            <tr bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/tsla.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://doi.org/10.1145/3712676.3714445" target="_blank">
                  <papertitle>LTS: A DASH Streaming System for Dynamic Multi-Layer 3D Gaussian Splatting Scenes</papertitle>
                <br>
                  <b style='color:red;'>Best Paper Award</b>.
                </a>
                <br>
                Yuan-Chun Sun, <strong>Yuang Shi</strong>, Cheng-Tse Lee, Mufeng Zhu, Wei Tsang Ooi, Yao Liu, Chun-Ying Huang, Cheng-Hsin Hsu
                <br>
                <em>The 16th ACM Multimedia Systems Conference (MMSys'25). </em>ACM, 2025.
                <br>
                <a href="https://dl.acm.org/doi/pdf/10.1145/3712676.3714445" target="_blank">Paper</a> / <a href="https://github.com/AIINS-NTHU/LTS-DASH-Streaming-System-for-3DGS" target="_blank">Code</a>
                <p>We develop, implement, and evaluate the very first DASH-based dynamic 3DGS streaming system, named Tile, Segment, and Layer Adaptive streaming (LTS). LTS is built on our previous work LapisGS, and achieves superior performance in both live streaming and on-demand streaming.</p>
                <br><br>
              </td>
            </tr>	


            <tr bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/lapis.png" alt="clean-usnob" width="150" height="130">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2408.14823" target="_blank">
                  <papertitle>LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Simone Gasparini, G√©raldine Morin, Wei Tsang Ooi, 
                <br>
                <em>The 12th International Conference on 3D Vision (3DV'25). </em>2025.
                <br>
                <a href="https://arxiv.org/abs/2408.14823" target="_blank">Paper</a> / <a href="https://github.com/nus-vv-streams/lapis-gs" target="_blank">Code</a> / <a href="https://yuang-ian.github.io/lapisgs/" target="_blank">Project Page</a>
                <p>We introduce LapisGS, a layered progressive 3D Gaussian Splatting (3DGS), which offers a progressive representation supporting a continuous rendering quality adapted for bandwidth-aware streaming.</p>
                <br><br>
              </td>
            </tr>	

            <tr bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/sketch_patch.png" alt="clean-usnob" width="180" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="https://doi.org/10.1145/3712677.3720466" target="_blank">
                  <papertitle>Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, G√©raldine Morin, Simone Gasparini, Chenggang Yang<sup>*</sup>, Wei Tsang Ooi
                <br>
                <em>MMSys Workshop. The 17th International Workshop on Immersive Mixed and Virtual Environment Systems (MMVE'25). </em>ACM, 2025. 
                <br>
                <strong>Rejected by MMSys'25 with 3/3 accepts from reviewrs, but I still love her.</strong>
                <br>
                <a href="images/mmsys25-mmve.pdf">Slides@MMVE</a> / <a href="https://doi.org/10.1145/3712677.3720466">Paper</a> / <a href="https://arxiv.org/abs/2501.13045" target="_blank">ArXiv (Full Version)</a>
                <!-- <a href="" target="_blank">Project Page</a> -->
                <p>Inspired by traditional painting techniques, we propose a novel hybrid representation for 3DGS that categorizes Gaussians into (i) Sketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians, which cover smooth regions. This hybrid categorization is conducive to efficient compression, and progressive and scalable streaming.</p>
                <br><br>
              </td>
            </tr>	

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gsvc.png" alt="clean-usnob" width="180" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://doi.org/10.1145/3712678.3721876" target="_blank">
                  <papertitle>GSVC: Efficient Video Representation and Compression Through 2D Gaussian Splatting</papertitle>
                </a>
                <br>
                Longan Wang<sup>*</sup>, <strong>Yuang Shi</strong>, Wei Tsang Ooi
                <br>
                <em>MMSys Workshop. The 35th edition of the Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV'25). </em>ACM, 2025. 
                <br>
                <a href="https://doi.org/10.1145/3712678.3721876">Paper</a> / <a href="https://arxiv.org/abs/2501.12060" target="_blank">ArXiv</a> / <a href="
                https://yuang-ian.github.io/gsvc/" target="_blank">Project Page</a> 
                <!-- <a href="" target="_blank">Project Page</a> -->
                <p>We propose GSVC, an approach to learning a set of 2D Gaussian splats that can effectively represent and compress video frames. Experiment results show that GSVC achieves good rate-distortion trade-offs, comparable to state-of-the-art video codecs such as AV1 and HEVC, and a rendering speed of 1500 fps for a 1920x1080 video.</p>
                <br><br>
              </td>
            </tr>	

            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmve_motion_vector.png" alt="clean-usnob" width="180" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="" target="_blank">
                  <papertitle>Joint Learning of Point Clouds and Motion Vectors for Volumetric Video</papertitle>
                </a>
                <br>
                Cheng-Tse Lee, Yuan-Chun Sun, <strong>Yuang Shi</strong>, Mufeng Zhu, Wei Tsang Ooi, Yao Liu, Chun-Ying Huang, and Cheng-Hsin Hsu
                <br>
                <em>MMSys Workshop. The 17th International Workshop on Immersive Mixed and Virtual Environment Systems (MMVE'25). </em>ACM, 2025. 
                <br>
                <a href="" target="_blank">Paper</a> /
                <p>We designed an end-to-end framework to jointly learn point clouds and motion vectors with augmented dynamic 3DGS training algorithms. Using the learned motion vectors for error concealment, we observe significant quality improvement compared to the SOTA neural-based method.</p>
                <br><br>
              </td>
            </tr>	 -->


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/panda.png" alt="clean-usnob" width="150" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/10.1145/3672196.3673394" target="_blank">
                  <papertitle>Multi-frame Bitrate Allocation of Dynamic 3D Gaussian Splatting Streaming Over Dynamic Networks</papertitle>
                <br>
                  <b style='color:red;'>Best Paper Award</b>.
                </a>
                <br>
                Yuan-Chun Sun, <strong>Yuang Shi</strong>, Wei Tsang Ooi, Chun-Ying Huang, Cheng-Hsin Hsu
                <br>
                <em>SIGCOMM Workshop. The 2024 SIGCOMM Workshop on Emerging Multimedia Systems (EMS'24). </em>ACM, 2024.
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3672196.3673394" target="_blank">Paper</a> / <a href="https://www.comp.nus.edu.sg/bytes/our-faculty-and-phd-student-win-best-paper-at-sigcomm-2024/" target="_blank">Press</a> from NUS.
                <p>We proposed two algorithms, MGA and MGAA, to allocate bitrate across multiple  3DGS scenes for streaming over dynamic networks.</p>
                <br><br>
              </td>
            </tr>	

            <tr bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmsys24.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/10.1145/3625468.3647619" target="_blank">
                  <papertitle>QV4: QoE-based Viewpoint-Aware V-PCC-encoded Volumetric Video Streaming</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Bennett Clement<sup>*</sup>, Wei Tsang Ooi
                <br>
                <em>The 15th ACM Multimedia Systems Conference (MMSys'24). </em>ACM, 2024.
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3625468.3647619" target="_blank">Paper</a> / <a href="https://github.com/nus-vv-streams/vvtk" target="_blank">Code</a>
                <p>We present QV4, a Quality-of-Experience (QoE) based streaming system for viewpoint-aware V-PCC-encoded volumetric video. QV4 achieves average compression ratio of <strong>610</strong> while keeping satisfactory quality, which is around <strong>80x</strong> better than other SOTA streaming systems.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmve-nerf.png" alt="clean-usnob" width="130" height="130">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/10.1145/3652212.3652220" target="_blank">
                  <papertitle>Volumetric Video Compression Through Neural-based Representation</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Ruoyu Zhao<sup>*</sup>, Simone Gasparini, Geraldine Morin, Wei Tsang Ooi
                <br>
                <em>MMSys Workshop. The 16th International Workshop on Immersive Mixed and Virtual Environment Systems (MMVE'24). </em>ACM, 2024.
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3652212.3652220" target="_blank">Paper</a> / <a href="https://github.com/nus-vv-streams/lc-checkpoint" target="_blank">Code</a>
                <p>We represent 3D dynamic content as a sequence of NeRFs, converting the explicit representation to neural representation. We then compress the neural representation based on the insight of significant similarity between successive NeRFs.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmve-qoe1.png" alt="clean-usnob" width="160" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/10.1145/3652212.3652218" target="_blank">
                  <papertitle>Quality Assessment and Modeling for MPEG V-PCC Volumetric Video</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Sam Cox, Wei Tsang Ooi
                <br>
                <em>MMSys Workshop. The 16th International Workshop on Immersive Mixed and Virtual Environment Systems (MMVE'24).</em>ACM, 2024.
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3652212.3652218" target="_blank">Paper</a> /
                <a href="https://github.com/nus-vv-streams/qoe-model" target="_blank">Dataset</a>
                <p>We propose a QoE model to predict the subjective quality with respect to the compression level of geometry and texture, quantifying the impact of geometry and texture compression on perceptual quality.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmve-qoe2.png" alt="clean-usnob" width="120" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/10.1145/3652212.3652221" target="_blank">
                  <papertitle>Perceptual Impact of Facial Quality in MPEG V-PCC-encoded Volumetric Videos</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Wei Tsang Ooi
                <br>
                <em>MMSys Workshop. The 16th International Workshop on Immersive Mixed and Virtual Environment Systems (MMVE'24). </em>ACM, 2024.
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3652212.3652221" target="_blank">Paper</a> /
                <a href="https://github.com/nus-vv-streams/facial-quality" target="_blank">Dataset</a>
                <p>We investigated the influence of rendering face quality of the avatars on users' viewing experience in MPEG V-PCC-encoded volumetric videos, and revealed the significant role of facial quality in influencing users' overall perceptual quality in volumetric videos.</p>
                <br><br>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmsys23_teaser_ref.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590981" target="_blank">
                  <papertitle>Enabling Low Bit-Rate MPEG V-PCC-encoded Volumetric Video Streaming with 3D Sub-sampling</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Pranav Venkatram<sup>*</sup>, Yifan Ding, Wei Tsang Ooi
                <br>
                <em>The 14th ACM Multimedia Systems Conference (MMSys'23). </em>ACM, 2023.
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590981" target="_blank">Paper</a> /
                <p>We show that it is possible to improve the quality of V-PCC encoded point clouds at low bit-rate by exploiting redundant information among the points in the 3D domain.</p>
                <br><br>
              </td>
            </tr>

            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmsys23_dataset.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3592546" target="_blank">
                  <papertitle>A Dynamic 3D Point Cloud Dataset for Immersive Applications</papertitle>
                </a>
                <br>
                Yuan-Chun Sun, I-Chun Huang, <strong>Yuang Shi</strong>, Wei Tsang Ooi, Chun-Ying Huang, Cheng-Hsin Hsu
                <br>
                <em>The 14th ACM Multimedia Systems Conference (MMSys'23). </em>ACM, 2023.
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3592546" target="_blank">Paper</a> /
                <p>We present an open dynamic 3D point cloud dataset with motion ground truth, which can be used by researchers who need temporal information across frames, e.g., motion estimation.</p>
                <br><br>
              </td>
            </tr>	 -->

          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p>
                  When I was a Master student, my dissertation is about human activity recognition in-the-wild. 
              </td>
            </tr>
          </tbody>
        </table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/shapecnf_architecture.png" alt="clean-usnob" width="160" height="70">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9767353" target="_blank">
                  <papertitle>Shape-Based Conditional Neural Field for Wrist-Worn Change-Point Detection</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Varsha Suresh, Wei Tsang Ooi
                <br>
                <em>The 2022 IEEE International Conference on Pervasive Computing and Communications Workshops. </em>IEEE, 2022.
                <br>
                <a href="https://drive.google.com/file/d/1cDNLQkChUhzaJeazOrpSmHLusLIt5JVz/view?usp=sharing" target="_blank">Paper</a> /
                <a href="https://github.com/Shiyuang-scu/ShapeCNF" target="_blank">Code</a>
                <p>ShapeCNF is a simple, fast, and accurate change-point detection method which uses shape-based features to model the patterns and a conditional neural field to model the temporal correlations among the time regions.</p>
                <br><br>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p>
                  During my undergraduate, I spent most of my time on medical image analysis.
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/npc_seg.png" alt="clean-usnob" width="160" height="110">
              </td>
              <td width="75%" valign="middle">
                <a href="https://doi.org/10.1016/j.knosys.2023.110598" target="_blank">
                  <papertitle>Uncertainty-weighted and Relation-driven Consistency Training for Semi-supervised Head-and-Neck Tumor Segmentation</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, et al.
                <br>
                <em>Knowledge-based Systems </em> (2023): 110598.
                <br>
                <a href="https://doi.org/10.1016/j.knosys.2023.110598" target="_blank">Paper</a> /
                <p>We propose a consistency training framework for semi-supervised NPC segmentation, which includes an Uncertainty-weighted Prediction Consistency Training (UPCT) strategy and a Relation-driven Consistency Training (RCT) strategy.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ASMFS.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://doi.org/10.1016/j.patcog.2022.108566" target="_blank">
                  <papertitle>ASMFS: Adaptive-Similarity-based Multi-modality Feature Selection for Classification of Alzheimer's Disease</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, et al.
                <br>
                <em>Pattern Recognition </em>126 (2022): 108566.
                <br>
                <a href="https://drive.google.com/file/d/1hG1PZC4ZAqyZw0hJRDqGrzqBxcDhVTHz/view?usp=sharing" target="_blank">Paper</a> /
                <a href="https://github.com/Shiyuang-scu/ASMFS" target="_blank">Code</a>
                <p>ASMFS is a novel multi-modal feature selection method for classification of Alzheimer's Disease, which performs adaptive similarity learning and feature selection simultaneously.</p>
                <br><br>
              </td>
            </tr>
          </tbody>
        </table>

				
					

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Teaching</heading>
                <p>
                  <ul>
                    <li> 
                      [2025/01-2025/05] - Graduate Tutor & Teaching Assistant, CS2106 Introduction to Operating Systems. 
                      <ul>
                        <li>
                          Over 600 students enrolled.
                        </li>
                        <li>
                          Conducted weekly tutorials.
                        </li>
                      </ul>
                    </li>  
                    <li> 
                      [2024/08-2024/12] - Graduate Tutor & Teaching Assistant, CS3244 Machine Learning.
                      <ul>
                        <li>
                          Over 100 students enrolled.
                        </li>
                        <li>
                          Conducted weekly tutorials; Graded assignments; Mentored course projects; Made Midterm and Final Exam.
                        </li>
                      </ul>
                    </li>                  
                    <li> 
                      [2024/01-2024/05] - Graduate Tutor & Teaching Assistant, CS3244 Machine Learning.
                      <ul>
                        <li>
                          Over 100 students enrolled.
                        </li>
                        <li>
                          Conducted weekly tutorials; Graded assignments; Mentored course projects; Made Midterm and Final Exam.
                        </li>
                      </ul>
                    </li>
                    <li>
                      [2023/01-2023/05] - Graduate Tutor & Teaching Assistant, CS3244 Machine Learning.
                      <ul>
                        <li>
                          Over 100 students enrolled.
                        </li>
                        <li>
                          Conducted weekly tutorials; Graded assignments; Mentored course projects; Made Midterm and Final Exam.
                        </li>
                      </ul>
                    </li>
                    <li>
                      [2022/08-2022/12] - Graduate Tutor & Teaching Assistant, CS3244 Machine Learning.
                      <ul>
                        <li>
                          Over 200 students enrolled.
                        </li>
                        <li>
                          Conducted weekly tutorials; Graded assignments; Mentored course projects; Made Midterm and Final Exam.
                        </li>
                      </ul>
                    </li>
                  </ul>
              </td>
            </tr>
          </tbody>
        </table>

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/photograph.jpg" alt="photo" width="160" height="120">
            </td>
            <td width="75%" valign="center">
              I love photography. Have a look at some of my <a href="https://flic.kr/ps/3WM9SL"> photos</a>
              <br><br>
            </td>
          </tr> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Mentored Students (Selected)</heading>
              <p>
                I really appreciate the opportunity to work with the following talent students (I only list the selected ones who I have worked with for a long time, and I have made non-negligible contributions to their research projects üòÉ.):
                <ol>
                  <li>
                    Jia Yi Lee.
                    <ul>
                      <li>
                        FYP 2025. Efficient Video Representation with 3D Gaussian Splatting.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <a href="https://longanwang-cs.github.io">Longan Wang</a> --> PhD @ NUS.
                    <ul>
                      <li>
                        Undergraduate NGNE 2025. Gaussian-based Video Compression.
                      </li>
                      <li>
                        Co-author of an MMSys'25 paper.
                      </li>
                    </ul>
                  </li>
                  <li>
                    Chenggang Yang --> Tencent.
                    <ul>
                      <li>
                        Master Dissertation 2024. 3D Line Segment Representation on 3D Gaussian Splatting.
                      </li>
                      <li>
                        Co-author of an MMSys'25 paper.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <a href="https://sg.linkedin.com/in/sherwin-poh">Sherwin Poh</a> --> Apple.
                    <ul>
                      <li>
                        FYP 2024. Real-time Point Cloud Super-Resolution for Volumetric Video Streaming.
                      </li>
                      <li>
                        Contributor of VVTk, an open-sourced toolkit for volumetric video streaming.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <a href="https://sg.linkedin.com/in/ngchisern">Chi Sern Ng</a> --> Tiktok.
                    <ul>
                      <li>
                        FYP 2024. Scalable Approaches to Volumetric Video Playback.
                      </li>
                      <li>
                        Contributor of VVTk, an open-sourced toolkit for volumetric video streaming.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <a href="https://sg.linkedin.com/in/guanlin-joseph-wang">Joseph Wang Guanlin</a> --> RA @ NUS.
                    <ul>
                      <li>
                        FYP 2024, UROP 2023. Improving the Performance of V-PCC through Segmentation With Meta's SAM.
                      </li>
                      <li>
                        Contributor of VVTk, an open-sourced toolkit for volumetric video streaming.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <a href="https://www.benclmnt.com">Bennett Clement</a> --> HoYoVerse.
                    <ul>
                      <li>
                        FYP 2023. Viewpoint-aware VPCC-Encoded Volumetric Video Streaming.
                      </li>
                      <li>
                        Co-author of an MMSys'24 paper.
                      </li>
                    </ul>
                  </li>
                  <li>
                    <a href="https://orcid.org/0009-0008-8170-4208">Ruoyu Zhao</a>.
                    <ul>
                      <li>
                        Undergraduate Research Intern 2023. Neural-based Representation for Volumetric Video Compression.
                    </li>
                    <li>
                      Co-author of an MMSys'24 paper.
                    </li>
                  </ul>
                  </li>
                  <li>
                    <a href="https://scholar.google.com/citations?user=pO9FC1oAAAAJ&hl=en">Shixin Ji</a> --> PhD @ Brown University.
                    <ul>
                      <li>
                        Undergraduate NGNE 2023. Human Activity Recognition In-the-Wild Using Wearable Devices.
                    </li>
                  </ul>
                  </li>
                </ol>
                <br>
                *FYP (Undergraduate Final Year Project), UROP (Undergraduate Research Opportunities Programme), NGNE (Non-Graduating Non-Exchange Programme).
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Activity amd Volunteer</heading>
              <p>
                <ul>
                  <li>
                    [2024/05] - Invited talk at Sichuan University, Sichuan.
                  </li>
                  <li>
                    [2024/04] - Invited talk at Universit√© de Toulouse, Toulouse INP-ENSEEIHT, IRIT, Toulouse.
                  </li>
                  <li>
                    [2023/12] - Invited <a href="https://dcs.site.nthu.edu.tw/p/406-1174-260551,r67.php" target="_blank">talk</a> at National Tsing Hua University, HsinChu.
                  </li>
                  <li>
                    [2022/11] - Student Volunteer. The 21st IEEE International Symposium on Mixed and Augmented Reality, 2022, Singapore.
                  </li>
                </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Peer Reviewer</heading>
              <p>
                <ul>
                  <li>
                    IEEE Multimedia.
                  </li>
                  <li>
                    Medical Image Analysis (MIA).
                  </li>
                  <li>
                    IEEE Transactions on Cognitive and Developmental Systems (TCDS).
                  </li>
                  <li>
                    ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM).
                  </li>
                  <li>
                    ACM Multimedia (ACM MM) 2024.
                  </li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This template comes from Jon Barron's public academic <a href="https://jonbarron.info/">website</a>. ‚ù§Ô∏è
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

<script>
  let text = document.lastModified;
  document.getElementById("demo").innerHTML = text;
</script>

</html>
